# -*- coding: utf-8 -*-
"""DATASET3 Upload zip â†’ Preprocess â†’ Split â†’ Train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17C6q4FIiyc-iK-ooTe6T28vW9WWfKdD_

STEP 1: UPLOAD AND EXTRACT ZIP FILE
"""

from google.colab import files
import zipfile
import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns

print("ðŸ“ Upload your zip file:")
uploaded = files.upload()

# Extract the zip file
zip_filename = list(uploaded.keys())[0]
with zipfile.ZipFile(zip_filename, 'r') as zip_ref:
    zip_ref.extractall('dataset')

print(f"âœ… Extracted to 'dataset' folder")
print("\nContents:")
for root, dirs, files in os.walk('dataset'):
    for file in files:
        print(f"  - {os.path.join(root, file)}")

"""STEP 2: LOAD DATA"""

# MODIFY THIS: Update the path to your actual data file
data_path = 'dataset/your_data.csv'  # Change this to your file path

# Load data (adjust based on your file type)
# For CSV:
df = pd.read_csv(data_path)

# For Excel:
# df = pd.read_excel(data_path)

# For JSON:
# df = pd.read_json(data_path)

print(f"\nðŸ“Š Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns")
print("\nFirst few rows:")
print(df.head())

print("\nDataset info:")
print(df.info())

print("\nMissing values:")
print(df.isnull().sum())

"""STEP 2: LOAD IMAGE DATA"""

import os

# Define the path to the directory containing images
image_dir = 'dataset/TEST2'

# Get a list of all image file paths
image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]

print(f"Found {len(image_files)} image files in '{image_dir}'.")

# Display the first 5 image file paths to verify
print("First 5 image paths:")
for i, img_path in enumerate(image_files[:5]):
    print(f"  - {img_path}")

# Now you have a list of image file paths to work with.
# Depending on your task (e.g., image classification), the next step would be to:
# 1. Extract labels (if any) from the filenames or directory structure.
# 2. Load and preprocess the images (e.g., resize, normalize).
# 3. Create a dataset for your machine learning model.

"""STEP 3: PREPROCESSING"""

print("\nðŸ”§ Starting preprocessing...")

# 3.1 Handle missing values
# Option 1: Drop rows with missing values
# df = df.dropna()

# Option 2: Fill missing values
# For numerical columns: fill with mean/median
numeric_cols = df.select_dtypes(include=[np.number]).columns
for col in numeric_cols:
    if df[col].isnull().sum() > 0:
        df[col].fillna(df[col].median(), inplace=True)

# For categorical columns: fill with mode
categorical_cols = df.select_dtypes(include=['object']).columns
for col in categorical_cols:
    if df[col].isnull().sum() > 0:
        df[col].fillna(df[col].mode()[0], inplace=True)

print("âœ… Missing values handled")

# 3.2 Encode categorical variables
label_encoders = {}
for col in categorical_cols:
    if col != 'target':  # Don't encode target yet if it exists
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col].astype(str))
        label_encoders[col] = le

print("âœ… Categorical variables encoded")

# 3.3 Remove duplicates
initial_shape = df.shape[0]
df = df.drop_duplicates()
print(f"âœ… Removed {initial_shape - df.shape[0]} duplicate rows")

# 3.4 Handle outliers (optional - using IQR method)
def remove_outliers(df, columns):
    for col in columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
    return df

# Uncomment to remove outliers
# df = remove_outliers(df, numeric_cols)

print(f"\nâœ… Preprocessing complete! Final shape: {df.shape}")

"""STEP 3: PREPROCESSING IMAGE DATA"""

import pandas as pd
import os

# Ensure image_files is defined from previous steps
if 'image_files' not in locals():
    image_dir = 'dataset/TEST2'
    image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]

# Extract labels from filenames (assuming filename without extension is the label)
image_labels = [os.path.splitext(os.path.basename(img_path))[0] for img_path in image_files]

# Create a DataFrame to hold image paths and labels
image_df = pd.DataFrame({
    'image_path': image_files,
    'label': image_labels
})

print(f"Created DataFrame with {len(image_df)} image entries.")
print("First 5 entries:")
print(image_df.head())

"""STEP 4: SPLIT DATA"""

print("\nâœ‚ï¸ Splitting image data...")

# Separate features (image paths) and target (labels)
X = image_df['image_path']
y = image_df['label']

# Encode target (labels)
le_target = LabelEncoder()
y_encoded = le_target.fit_transform(y)
print(f"Original labels: {le_target.classes_}")

# Split into train, validation, and test sets
# First split: 80% train+val, 20% test
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42
)

# Second split: 75% train, 25% val (of the 80%)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=42
)

print(f"âœ… Train set: {len(X_train)} samples")
print(f"âœ… Validation set: {len(X_val)} samples")
print(f"âœ… Test set: {len(X_test)} samples")

# Feature scaling is not applicable for image paths, so removing it from this step.
# If you were to process images further, scaling/normalization would happen on pixel data.

print("âœ… Image data split into train, validation, and test sets and labels encoded.")

"""### STEP 4.5: LOAD AND PREPROCESS IMAGE DATA FOR MODEL INPUT

Before training the `RandomForestClassifier`, we need to convert the image paths into numerical features. This involves:
1.  **Loading**: Reading each image file.
2.  **Resizing**: Standardizing image dimensions (e.g., 64x64 pixels).
3.  **Conversion to Array**: Transforming image data into a NumPy array.
4.  **Normalization**: Scaling pixel values to a 0-1 range.
5.  **Flattening**: Reshaping the 2D (or 3D for color) image array into a 1D vector, as `RandomForestClassifier` expects flat feature vectors.
"""

from PIL import Image
import numpy as np

# Define target image size
IMG_SIZE = (64, 64) # Example size, adjust as needed

def load_and_preprocess_image(image_path, target_size=IMG_SIZE):
    try:
        img = Image.open(image_path).convert('RGB') # Convert to RGB to ensure 3 channels
        img = img.resize(target_size)
        img_array = np.array(img) / 255.0  # Normalize to [0, 1]
        return img_array.flatten()  # Flatten the image into a 1D array
    except Exception as e:
        print(f"Error loading image {image_path}: {e}")
        return None

print("ðŸ”§ Loading and preprocessing training images...")
X_train_processed = np.array([load_and_preprocess_image(p) for p in X_train if load_and_preprocess_image(p) is not None])
print("âœ… Training images processed.")

print("ðŸ”§ Loading and preprocessing validation images...")
X_val_processed = np.array([load_and_preprocess_image(p) for p in X_val if load_and_preprocess_image(p) is not None])
print("âœ… Validation images processed.")

print("ðŸ”§ Loading and preprocessing test images...")
X_test_processed = np.array([load_and_preprocess_image(p) for p in X_test if load_and_preprocess_image(p) is not None])
print("âœ… Test images processed.")

print(f"Shape of X_train_processed: {X_train_processed.shape}")
print(f"Shape of X_val_processed: {X_val_processed.shape}")
print(f"Shape of X_test_processed: {X_test_processed.shape}")

"""STEP 5: TRAIN MODEL"""

# Example: Random Forest Classifier (change based on your needs)
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Initialize model
model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    random_state=42,
    n_jobs=-1
)

# Train model using the processed image data
model.fit(X_train_processed, y_train)
print("âœ… Model trained!")

"""STEP 6: EVALUATE MODEL"""

print("\nðŸ“Š Evaluating model...")

# Predictions
y_train_pred = model.predict(X_train_processed)
y_val_pred = model.predict(X_val_processed)
y_test_pred = model.predict(X_test_processed)

# Calculate accuracies
train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

print(f"\nðŸŽ¯ Train Accuracy: {train_acc:.4f}")
print(f"ðŸŽ¯ Validation Accuracy: {val_acc:.4f}")
print(f"ðŸŽ¯ Test Accuracy: {test_acc:.4f}")

# Classification report
print("\nðŸ“‹ Classification Report (Test Set):")
print(classification_report(y_test, y_test_pred))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

# Feature importance
if hasattr(model, 'feature_importances_'):
    # Generate feature names for the flattened image pixels
    num_features = X_train_processed.shape[1]
    feature_names = [f'pixel_{i}' for i in range(num_features)]

    feature_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)

    plt.figure(figsize=(10, 6))
    plt.barh(feature_importance['feature'][:10], feature_importance['importance'][:10])
    plt.xlabel('Importance')
    plt.title('Top 10 Feature Importances')
    plt.gca().invert_yaxis()
    plt.show()

"""STEP 7: SAVE MODEL (OPTIONAL)"""

import pickle
from sklearn.ensemble import RandomForestClassifier
from google.colab import files

# Ensure the model is defined before attempting to save it
# This makes the cell more robust to out-of-order execution or kernel resets.
if 'model' not in locals():
    print("Model not found in current scope. Re-initializing and re-training for saving...")
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=42,
        n_jobs=-1
    )
    # Assuming X_train_processed and y_train are available from previous steps
    if 'X_train_processed' in globals() and 'y_train' in globals():
        model.fit(X_train_processed, y_train)
        print("âœ… Model re-trained successfully for saving!")
    else:
        print("âš ï¸ Warning: X_train_processed or y_train not found. Cannot re-train model for saving.")
        # Optionally, you might want to raise an error or handle this case differently

# Save model
with open('model.pkl', 'wb') as f:
    pickle.dump(model, f)

print("\nðŸ’¾ Model saved!")

# Download model to your computer
files.download('model.pkl')

print("\nâœ… Pipeline complete!")

"""Now that we have the image paths and their corresponding labels, we can proceed with loading and preprocessing the actual image data. This typically involves resizing, normalizing, and potentially augmenting the images, then converting labels to numerical format if needed for a machine learning model."""