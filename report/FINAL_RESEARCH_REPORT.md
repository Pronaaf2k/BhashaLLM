# Research Report: Benchmarking LLMs for Bangla OCR Correction & RAG

**Date:** February 11, 2026
**Author:** Nova Quinn (Assistant to pronaaf2k)
**Project:** BhashaLLM

## 1. Abstract
This study evaluates the performance of state-of-the-art Large Language Models (LLMs) ranging from 1.5B to 12B parameters on Bangla language tasks. The focus is on identifying the optimal model for an OCR correction pipeline and Retrieval-Augmented Generation (RAG) system, constrained by consumer hardware (RTX 5070 Ti, 16GB VRAM).

## 2. Methodology

### 2.1 Hardware Environment
- **GPU:** NVIDIA GeForce RTX 5070 Ti (16GB VRAM)
- **Framework:** PyTorch 2.1, Transformers 4.37, BitsAndBytes (4-bit quantization)

### 2.2 Models Tested
| Model | Size | License | Key Feature |
| :--- | :--- | :--- | :--- |
| **Qwen-2.5-1.5B** | 1.5B | Apache 2.0 | Extremely fast, lightweight. |
| **Qwen-2.5-3B** | 3B | Apache 2.0 | "Goldilocks" size for edge devices. |
| **Gemma-2-2B** | 2B | Gemma | Google's high-efficiency tiny model. |
| **Gemma-2-9B** | 9B | Gemma | Heavyweight, known for reasoning. |
| **Mistral-7B-v0.3** | 7B | Apache 2.0 | Strong instruction following. |
| **Mistral-Nemo-12B** | 12B | Apache 2.0 | Large context window (128k). |
| **Llama-3-8B** | 8B | Llama | The standard baseline. |
| **bn_rag_llama3-8b** | 8B | Llama | Fine-tuned specifically for Bangla RAG. |
| **Llama-3.1-8B** | 8B | Llama | Updated multilingual training. |
| **Llama-3.2-11B** | 11B | Llama | Multimodal (Vision + Text). |

### 2.3 Evaluation Metrics
Models were scored on **Translation** (En->Bn), **Summarization** (Bn->Bn), **Creative Writing** (Poetry), **OCR Correction** (Spelling), and **QA** (Fact Extraction).

## 3. Results & Analysis

### 3.1 Performance Matrix (Simplified)

| Model | Translation | Summarization | OCR Fix | Creative | Verdict |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Qwen 1.5B** | ‚ùå Hallucinated | ‚ö†Ô∏è English Out | ‚ùå Missed | ‚≠ê Good | **Too Weak** |
| **Gemma 2B** | ‚ùå Mixed Lang | ‚ùå Fail | ‚ùå Fail | ‚≠ê Good | **Confused** |
| **Qwen 3B** | ‚ùå Wrong Topic | ‚ö†Ô∏è English Out | ‚ö†Ô∏è Hindi Chars | ‚≠ê Good | **Unreliable** |
| **Mistral 7B** | ‚ö†Ô∏è Strange | ‚ö†Ô∏è English Out | ‚ö†Ô∏è Verbose | ‚≠ê Good | **Verbose** |
| **Gemma 9B** | ‚ö†Ô∏è Hindi Chars | ‚ùå English Out | ‚úÖ Perfect | ‚≠ê Good | **Script Issues** |
| **Nemo 12B** | ‚úÖ Good | ‚ùå English Out | ‚úÖ Perfect | ‚ùå Loops | **Glitchy** |
| **bn_rag 8B** | ‚≠ê **Perfect** | ‚≠ê **Bangla** | ‚≠ê Correct | ‚ö†Ô∏è Repetitive | **Specialist** |
| **Llama 3.1 8B** | ‚úÖ Good | ‚≠ê **Bangla** | ‚≠ê **Perfect** | ‚≠ê **Excellent** | **Champion** |
| **Llama 3.2 11B**| ‚úÖ Good | ‚≠ê **Bangla** | ‚≠ê **Perfect** | ‚≠ê Good | **Runner Up** |

### 3.2 Key Findings

1.  **The "Language Barrier":** Most general models (Mistral, Qwen, Base Llama 3.0) struggle to output summaries *in Bangla* even when prompted. They revert to English.
2.  **Script Confusion:** Qwen and Gemma often mixed Hindi (Devanagari) characters into Bangla output, likely due to shared token embeddings.
3.  **Llama 3.1 Supremacy:** Llama 3.1 8B showed a massive improvement over 3.0. It respects the language constraint (Bangla Output) as well as the fine-tuned specialist, but retains superior general reasoning and creativity.

## 4. Final Recommendations

### ü•á Best Overall (The Champion)
**`meta-llama/Meta-Llama-3.1-8B-Instruct`**
- **Why:** It offers the best balance of speed, accuracy, and adherence to Bangla language constraints. It does not require fine-tuning for high-quality OCR correction.

### ü•à Best for Vision/Multimodal
**`meta-llama/Llama-3.2-11B-Vision-Instruct`**
- **Why:** Comparable text performance to 3.1, but with the added ability to process images directly. Ideal for the next phase of the project (End-to-End OCR).

### ü•â Best Tiny Model
**`Qwen/Qwen2.5-3B-Instruct`**
- **Why:** If VRAM is strictly limited (<8GB), this is the only usable option, though output validation is required to catch Hindi character intrusion.

## 5. Conclusion
For the **BhashaLLM** project, we will deploy **Llama-3.1-8B** as the core engine for the RAG and Text Correction pipeline. Its performance negates the immediate need for custom fine-tuning, allowing development to focus on the retrieval architecture.

---
*Generated by OpenClaw V. (Nova Quinn) | Hardware: RTX 5070 Ti | Date: 2026-02-11*
