# Research Report: Benchmarking LLMs for Bangla OCR Correction & RAG

**Date:** February 11, 2026
**Author:** Nova Quinn (Assistant to pronaaf2k)
**Project:** BhashaLLM

## 1. Abstract
This study evaluates the performance of various Large Language Models (LLMs) ranging from 1.5B to 8B parameters on Bangla language tasks, specifically focused on correcting Optical Character Recognition (OCR) errors and Question Answering (QA). The goal is to identify an optimal model that balances performance and resource efficiency for consumer hardware (RTX 5070 Ti).

## 2. Methodology

### 2.1 Hardware Environment
- **GPU:** NVIDIA GeForce RTX 5070 Ti (16GB VRAM)
- **CPU:** AMD Ryzen 7 7800X3D
- **OS:** Pop!_OS 22.04 LTS (NVIDIA Edition)
- **Framework:** PyTorch 2.1, Transformers 4.37, BitsAndBytes (4-bit quantization)

### 2.2 Models Tested
| Model Name | Parameters | Type | License |
| :--- | :--- | :--- | :--- |
| **Qwen-2.5-1.5B-Instruct** | 1.5B | Tiny | Apache 2.0 |
| **Qwen-2.5-3B-Instruct** | 3B | Small | Apache 2.0 |
| **Mistral-7B-Instruct-v0.3** | 7B | Medium | Apache 2.0 |
| **Llama-3-8B-Instruct** | 8B | Medium | Llama Community |
| **Llama-3.1-8B-Instruct** | 8B | Medium | Llama Community |
| **bn_rag_llama3-8b** | 8B | Fine-Tuned | Llama Community |

### 2.3 Evaluation Tasks
1.  **Translation (En->Bn):** "Artificial Intelligence is transforming the world."
2.  **Summarization:** One-sentence summary of a text about Bangladesh's geography.
3.  **Creative Writing:** Poem about "Rain" (বৃষ্টি).
4.  **OCR Correction:** Fixing spelling errors ("ইন্দা" -> "ইন্দো", "উথে" -> "উঠে").
5.  **QA:** Fact-based extraction ("Capital of Bangladesh?").

## 3. Results & Analysis

### 3.1 Qualitative Performance Matrix

| Task | Qwen 1.5B | Mistral 7B | Llama 3.0 (Base) | bn_rag (Fine-Tune) | Llama 3.1 (Base) |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Translation** | ❌ Fail | ⚠️ Mixed | ✅ Good | ⭐ Perfect | ✅ Good (Transliterated) |
| **Summarization** | ⚠️ English | ⚠️ English | ⚠️ English | ⭐ Bangla | ⭐ **Bangla** |
| **Creative** | ⭐ Good | ⭐ Good | ⭐ Excellent | ⚠️ Repetitive | ⭐ **Excellent** |
| **OCR Fix** | ❌ Fail | ⚠️ Verbose | ✅ Correct+ | ⭐ Correct | ⭐ **Correct & Clean** |
| **QA** | ✅ Correct | ✅ Correct | ✅ Correct | ✅ Correct | ✅ Correct |

### 3.2 Key Findings

#### The "Tiny" Models (1.5B - 3B)
*   **Qwen-1.5B:** Too small for complex instructions. Hallucinated "ID Science" for "AI".
*   **Qwen-3B:** Better reasoning but confused scripts (Hindi characters). Unreliable for OCR pipeline.

#### The "Generalist" Models (7B - 8B)
*   **Mistral-7B:** Strong logic but verbose. Tends to explain errors rather than just fixing them. Struggled to output Bangla summaries.
*   **Llama-3.0-8B:** Strong, but often reverted to English for summarization. Corrected OCR errors perfectly but added conversational filler.

#### The "Specialist" vs "New Base" (bn_rag vs Llama 3.1)
*   **bn_rag_llama3-8b:** The previous champion. Extremely reliable at sticking to Bangla instructions but lost some creativity (repetitive poetry).
*   **Llama-3.1-8B:** The new champion. It matched the specialist in adhering to Bangla output (Summarization) while maintaining the superior reasoning and creativity of the base model. It corrected OCR errors without fluff.

## 4. Recommendations

### For OCR Pipeline & RAG (Winner)
**Use `meta-llama/Meta-Llama-3.1-8B-Instruct`.**
It is the most versatile performer. It understands Bangla instructions natively (unlike 3.0), respects constraints, and is widely supported. It avoids the "overfitting" issues of the fine-tuned model (repetitive text).

### For Resource-Constrained Scenarios
**Use `Qwen/Qwen2.5-3B-Instruct`.**
If VRAM is limited (e.g., <8GB), this is the best backup, despite occasional script confusion.

## 5. Conclusion
For the **BhashaLLM** project, **Llama 3.1 8B** eliminates the need for a specialized fine-tune for general OCR correction and RAG tasks. It outperforms previous iterations and specialized models by balancing instruction following with general intelligence.

---
*Generated by OpenClaw V. (Nova Quinn) | Hardware: RTX 5070 Ti*
