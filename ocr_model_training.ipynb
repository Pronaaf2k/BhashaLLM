{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bangla Handwritten OCR Model Training\n",
        "\n",
        "This notebook trains and tests the OCR model using datasets from `banglaWrittenWordOCR-main`.\n",
        "\n",
        "## Pipeline:\n",
        "1. **Detection**: YOLOv8 for character detection\n",
        "2. **Recognition**: ResNet34 for character recognition (grapheme root + vowel diacritic + consonant diacritic)\n",
        "3. **Spelling Correction**: Word2Vec for post-processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision pillow opencv-python ultralytics pandas numpy gensim pretrainedmodels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image, ImageOps, ImageEnhance\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import json\n",
        "import warnings\n",
        "from ultralytics import YOLO\n",
        "import sys\n",
        "\n",
        "# Add paths\n",
        "sys.path.append('banglaWrittenWordOCR-main')\n",
        "sys.path.append('banglaWrittenWordOCR-main/recongnition_model')\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Dataset and Create DataLoaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset class for BanglaGrapheme dataset\n",
        "class BanglaGraphemeDataset:\n",
        "    def __init__(self, img_H, img_W, type='train', data_path='banglaWrittenWordOCR-main/recongnition_model/data/BanglaGrapheme'):\n",
        "        csv_path = os.path.join(data_path, f'{type}.csv')\n",
        "        if not os.path.exists(csv_path):\n",
        "            print(f\"Warning: {csv_path} not found. Using empty dataset.\")\n",
        "            self.image_ids = []\n",
        "            self.grapheme_root = []\n",
        "            self.vowel_diacritic = []\n",
        "            self.consonant_diacritic = []\n",
        "        else:\n",
        "            df = pd.read_csv(csv_path)\n",
        "            df = df[['image_id', 'grapheme_root', 'vowel_diacritic', 'consonant_diacritic']]\n",
        "            \n",
        "            self.image_ids = df.image_id.values\n",
        "            self.grapheme_root = df.grapheme_root.values\n",
        "            self.vowel_diacritic = df.vowel_diacritic.values\n",
        "            self.consonant_diacritic = df.consonant_diacritic.values\n",
        "        \n",
        "        self.width = img_W\n",
        "        self.height = img_H\n",
        "        self.type = type\n",
        "        self.data_path = data_path\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        image_folder = os.path.join(self.data_path, self.type)\n",
        "        image_path = os.path.join(image_folder, f\"{self.image_ids[item]}.jpg\")\n",
        "        \n",
        "        if not os.path.exists(image_path):\n",
        "            # Return a blank image if file doesn't exist\n",
        "            image = Image.new('RGB', (self.width, self.height), color='white')\n",
        "        else:\n",
        "            image = Image.open(image_path)\n",
        "            image = image.resize((self.width, self.height))\n",
        "            image = image.convert('RGB')\n",
        "        \n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "        \n",
        "        image = transform(image)\n",
        "        return {\n",
        "            'image': image,\n",
        "            'grapheme_root': torch.tensor(self.grapheme_root[item], dtype=torch.long),\n",
        "            'vowel_diacritic': torch.tensor(self.vowel_diacritic[item], dtype=torch.long),\n",
        "            'consonant_diacritic': torch.tensor(self.consonant_diacritic[item], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "IMG_H, IMG_W = 128, 224\n",
        "\n",
        "def create_dataloaders():\n",
        "    train_dataset = BanglaGraphemeDataset(IMG_H, IMG_W, 'train')\n",
        "    val_dataset = BanglaGraphemeDataset(IMG_H, IMG_W, 'val')\n",
        "    test_dataset = BanglaGraphemeDataset(IMG_H, IMG_W, 'test')\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "    \n",
        "    print(f\"Train samples: {len(train_dataset)}\")\n",
        "    print(f\"Val samples: {len(val_dataset)}\")\n",
        "    print(f\"Test samples: {len(test_dataset)}\")\n",
        "    \n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "train_loader, val_loader, test_loader = create_dataloaders()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load ResNet34 Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import ResNet34 model\n",
        "try:\n",
        "    from recongnition_model.models.model import resnet34\n",
        "    print(\"Loaded model from recongnition_model\")\n",
        "except:\n",
        "    try:\n",
        "        import pretrainedmodels\n",
        "        import torch.nn.functional as F\n",
        "        \n",
        "        class resnet34(nn.Module):\n",
        "            def __init__(self):\n",
        "                super(resnet34, self).__init__()\n",
        "                self.model = pretrainedmodels.__dict__[\"resnet34\"](pretrained=None)\n",
        "                self.l0 = nn.Linear(512, 168)  # grapheme_root\n",
        "                self.l1 = nn.Linear(512, 11)   # vowel_diacritic\n",
        "                self.l2 = nn.Linear(512, 7)    # consonant_diacritic\n",
        "\n",
        "            def forward(self, x):\n",
        "                bs, _, _, _ = x.shape\n",
        "                x = self.model.features(x)\n",
        "                x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n",
        "                l0 = self.l0(x)\n",
        "                l1 = self.l1(x)\n",
        "                l2 = self.l2(x)\n",
        "                return l0, l1, l2\n",
        "        print(\"Created model from scratch\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        print(\"Please ensure pretrainedmodels is installed: pip install pretrainedmodels\")\n",
        "\n",
        "# Initialize model\n",
        "model = resnet34().to(device)\n",
        "print(f\"Model initialized on {device}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load Grapheme Mappings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load grapheme mappings\n",
        "mapping_file = 'bangla_ocr_pipeline/grapheme_maps.json'\n",
        "if os.path.exists(mapping_file):\n",
        "    with open(mapping_file, 'r', encoding='utf-8') as f:\n",
        "        grapheme_maps = json.load(f)\n",
        "    print(\"Loaded grapheme mappings\")\n",
        "    print(f\"Grapheme roots: {len(grapheme_maps['grapheme_root'])}\")\n",
        "    print(f\"Vowel diacritics: {len(grapheme_maps['vowel_diacritic'])}\")\n",
        "    print(f\"Consonant diacritics: {len(grapheme_maps['consonant_diacritic'])}\")\n",
        "else:\n",
        "    print(f\"Warning: {mapping_file} not found. Creating default mappings.\")\n",
        "    grapheme_maps = {\n",
        "        \"grapheme_root\": {str(i): f\"root_{i}\" for i in range(168)},\n",
        "        \"vowel_diacritic\": {str(i): f\"vowel_{i}\" for i in range(11)},\n",
        "        \"consonant_diacritic\": {str(i): f\"cons_{i}\" for i in range(7)}\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss function\n",
        "def loss_fc(outputs, targets):\n",
        "    out1, out2, out3 = outputs\n",
        "    t1, t2, t3 = targets\n",
        "    \n",
        "    loss1 = nn.CrossEntropyLoss()(out1, t1)\n",
        "    loss2 = nn.CrossEntropyLoss()(out2, t2)\n",
        "    loss3 = nn.CrossEntropyLoss()(out3, t3)\n",
        "    \n",
        "    return (loss1 + loss2 + loss3) / 3\n",
        "\n",
        "# Optimizer and scheduler\n",
        "LEARNING_RATE = 1e-2\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
        ")\n",
        "\n",
        "# Multi-GPU support\n",
        "if torch.cuda.device_count() > 1:\n",
        "    model = nn.DataParallel(model)\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 5  # Adjust as needed\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        try:\n",
        "            image = data[\"image\"].to(device, dtype=torch.float)\n",
        "            grapheme_root = data[\"grapheme_root\"].to(device, dtype=torch.long)\n",
        "            vowel_diacritic = data[\"vowel_diacritic\"].to(device, dtype=torch.long)\n",
        "            consonant_diacritic = data[\"consonant_diacritic\"].to(device, dtype=torch.long)\n",
        "            \n",
        "            targets = (grapheme_root, vowel_diacritic, consonant_diacritic)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(image)\n",
        "            total_loss = loss_fc(outputs, targets)\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += total_loss.item()\n",
        "            num_batches += 1\n",
        "            \n",
        "            if (batch_idx + 1) % 100 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Batch {batch_idx+1}, Loss: {total_loss.item():.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch {batch_idx}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    avg_train_loss = running_loss / max(num_batches, 1)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    \n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_batches = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data in val_loader:\n",
        "            try:\n",
        "                image = data[\"image\"].to(device, dtype=torch.float)\n",
        "                grapheme_root = data[\"grapheme_root\"].to(device, dtype=torch.long)\n",
        "                vowel_diacritic = data[\"vowel_diacritic\"].to(device, dtype=torch.long)\n",
        "                consonant_diacritic = data[\"consonant_diacritic\"].to(device, dtype=torch.long)\n",
        "                \n",
        "                targets = (grapheme_root, vowel_diacritic, consonant_diacritic)\n",
        "                outputs = model(image)\n",
        "                loss = loss_fc(outputs, targets)\n",
        "                \n",
        "                val_loss += loss.item()\n",
        "                val_batches += 1\n",
        "            except Exception as e:\n",
        "                continue\n",
        "    \n",
        "    avg_val_loss = val_loss / max(val_batches, 1)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    \n",
        "    scheduler.step(avg_val_loss)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Save checkpoint\n",
        "    os.makedirs('models', exist_ok=True)\n",
        "    checkpoint = {\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state_dict': model.module.state_dict() if hasattr(model, 'module') else model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses\n",
        "    }\n",
        "    torch.save(checkpoint, f'models/ocr_model_epoch_{epoch+1}.pth')\n",
        "    print(f\"Saved checkpoint: models/ocr_model_epoch_{epoch+1}.pth\")\n",
        "\n",
        "print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Save Final Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the final model\n",
        "os.makedirs('models', exist_ok=True)\n",
        "if hasattr(model, 'module'):\n",
        "    torch.save(model.module.state_dict(), 'models/ocr_model_final.pth')\n",
        "else:\n",
        "    torch.save(model.state_dict(), 'models/ocr_model_final.pth')\n",
        "print(\"Model saved to models/ocr_model_final.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bangla Handwritten OCR Model Training\n",
        "\n",
        "This notebook trains and tests the OCR model using datasets from `banglaWrittenWordOCR-main`.\n",
        "\n",
        "## Pipeline:\n",
        "1. **Detection**: YOLOv8 for character detection\n",
        "2. **Recognition**: ResNet34 for character recognition (grapheme root + vowel diacritic + consonant diacritic)\n",
        "3. **Spelling Correction**: Word2Vec for post-processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision pillow opencv-python ultralytics pandas numpy gensim pretrainedmodels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image, ImageOps, ImageEnhance\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import json\n",
        "import warnings\n",
        "from ultralytics import YOLO\n",
        "import sys\n",
        "\n",
        "# Add paths\n",
        "sys.path.append('banglaWrittenWordOCR-main')\n",
        "sys.path.append('banglaWrittenWordOCR-main/recongnition_model')\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Dataset and Create DataLoaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset class for BanglaGrapheme dataset\n",
        "class BanglaGraphemeDataset:\n",
        "    def __init__(self, img_H, img_W, type='train', data_path='banglaWrittenWordOCR-main/recongnition_model/data/BanglaGrapheme'):\n",
        "        csv_path = os.path.join(data_path, f'{type}.csv')\n",
        "        if not os.path.exists(csv_path):\n",
        "            print(f\"Warning: {csv_path} not found. Using empty dataset.\")\n",
        "            self.image_ids = []\n",
        "            self.grapheme_root = []\n",
        "            self.vowel_diacritic = []\n",
        "            self.consonant_diacritic = []\n",
        "        else:\n",
        "            df = pd.read_csv(csv_path)\n",
        "            df = df[['image_id', 'grapheme_root', 'vowel_diacritic', 'consonant_diacritic']]\n",
        "            \n",
        "            self.image_ids = df.image_id.values\n",
        "            self.grapheme_root = df.grapheme_root.values\n",
        "            self.vowel_diacritic = df.vowel_diacritic.values\n",
        "            self.consonant_diacritic = df.consonant_diacritic.values\n",
        "        \n",
        "        self.width = img_W\n",
        "        self.height = img_H\n",
        "        self.type = type\n",
        "        self.data_path = data_path\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        image_folder = os.path.join(self.data_path, self.type)\n",
        "        image_path = os.path.join(image_folder, f\"{self.image_ids[item]}.jpg\")\n",
        "        \n",
        "        if not os.path.exists(image_path):\n",
        "            # Return a blank image if file doesn't exist\n",
        "            image = Image.new('RGB', (self.width, self.height), color='white')\n",
        "        else:\n",
        "            image = Image.open(image_path)\n",
        "            image = image.resize((self.width, self.height))\n",
        "            image = image.convert('RGB')\n",
        "        \n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "        \n",
        "        image = transform(image)\n",
        "        return {\n",
        "            'image': image,\n",
        "            'grapheme_root': torch.tensor(self.grapheme_root[item], dtype=torch.long),\n",
        "            'vowel_diacritic': torch.tensor(self.vowel_diacritic[item], dtype=torch.long),\n",
        "            'consonant_diacritic': torch.tensor(self.consonant_diacritic[item], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "IMG_H, IMG_W = 128, 224\n",
        "\n",
        "def create_dataloaders():\n",
        "    train_dataset = BanglaGraphemeDataset(IMG_H, IMG_W, 'train')\n",
        "    val_dataset = BanglaGraphemeDataset(IMG_H, IMG_W, 'val')\n",
        "    test_dataset = BanglaGraphemeDataset(IMG_H, IMG_W, 'test')\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "    \n",
        "    print(f\"Train samples: {len(train_dataset)}\")\n",
        "    print(f\"Val samples: {len(val_dataset)}\")\n",
        "    print(f\"Test samples: {len(test_dataset)}\")\n",
        "    \n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "train_loader, val_loader, test_loader = create_dataloaders()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load ResNet34 Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import ResNet34 model\n",
        "try:\n",
        "    from recongnition_model.models.model import resnet34\n",
        "    print(\"Loaded model from recongnition_model\")\n",
        "except:\n",
        "    try:\n",
        "        import pretrainedmodels\n",
        "        import torch.nn.functional as F\n",
        "        \n",
        "        class resnet34(nn.Module):\n",
        "            def __init__(self):\n",
        "                super(resnet34, self).__init__()\n",
        "                self.model = pretrainedmodels.__dict__[\"resnet34\"](pretrained=None)\n",
        "                self.l0 = nn.Linear(512, 168)  # grapheme_root\n",
        "                self.l1 = nn.Linear(512, 11)   # vowel_diacritic\n",
        "                self.l2 = nn.Linear(512, 7)    # consonant_diacritic\n",
        "\n",
        "            def forward(self, x):\n",
        "                bs, _, _, _ = x.shape\n",
        "                x = self.model.features(x)\n",
        "                x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n",
        "                l0 = self.l0(x)\n",
        "                l1 = self.l1(x)\n",
        "                l2 = self.l2(x)\n",
        "                return l0, l1, l2\n",
        "        print(\"Created model from scratch\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        print(\"Please ensure pretrainedmodels is installed: pip install pretrainedmodels\")\n",
        "\n",
        "# Initialize model\n",
        "model = resnet34().to(device)\n",
        "print(f\"Model initialized on {device}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load Grapheme Mappings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load grapheme mappings\n",
        "mapping_file = 'bangla_ocr_pipeline/grapheme_maps.json'\n",
        "if os.path.exists(mapping_file):\n",
        "    with open(mapping_file, 'r', encoding='utf-8') as f:\n",
        "        grapheme_maps = json.load(f)\n",
        "    print(\"Loaded grapheme mappings\")\n",
        "    print(f\"Grapheme roots: {len(grapheme_maps['grapheme_root'])}\")\n",
        "    print(f\"Vowel diacritics: {len(grapheme_maps['vowel_diacritic'])}\")\n",
        "    print(f\"Consonant diacritics: {len(grapheme_maps['consonant_diacritic'])}\")\n",
        "else:\n",
        "    print(f\"Warning: {mapping_file} not found. Creating default mappings.\")\n",
        "    grapheme_maps = {\n",
        "        \"grapheme_root\": {str(i): f\"root_{i}\" for i in range(168)},\n",
        "        \"vowel_diacritic\": {str(i): f\"vowel_{i}\" for i in range(11)},\n",
        "        \"consonant_diacritic\": {str(i): f\"cons_{i}\" for i in range(7)}\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss function\n",
        "def loss_fc(outputs, targets):\n",
        "    out1, out2, out3 = outputs\n",
        "    t1, t2, t3 = targets\n",
        "    \n",
        "    loss1 = nn.CrossEntropyLoss()(out1, t1)\n",
        "    loss2 = nn.CrossEntropyLoss()(out2, t2)\n",
        "    loss3 = nn.CrossEntropyLoss()(out3, t3)\n",
        "    \n",
        "    return (loss1 + loss2 + loss3) / 3\n",
        "\n",
        "# Optimizer and scheduler\n",
        "LEARNING_RATE = 1e-2\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
        ")\n",
        "\n",
        "# Multi-GPU support\n",
        "if torch.cuda.device_count() > 1:\n",
        "    model = nn.DataParallel(model)\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 5  # Adjust as needed\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        try:\n",
        "            image = data[\"image\"].to(device, dtype=torch.float)\n",
        "            grapheme_root = data[\"grapheme_root\"].to(device, dtype=torch.long)\n",
        "            vowel_diacritic = data[\"vowel_diacritic\"].to(device, dtype=torch.long)\n",
        "            consonant_diacritic = data[\"consonant_diacritic\"].to(device, dtype=torch.long)\n",
        "            \n",
        "            targets = (grapheme_root, vowel_diacritic, consonant_diacritic)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(image)\n",
        "            total_loss = loss_fc(outputs, targets)\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += total_loss.item()\n",
        "            num_batches += 1\n",
        "            \n",
        "            if (batch_idx + 1) % 100 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Batch {batch_idx+1}, Loss: {total_loss.item():.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch {batch_idx}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    avg_train_loss = running_loss / max(num_batches, 1)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    \n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_batches = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data in val_loader:\n",
        "            try:\n",
        "                image = data[\"image\"].to(device, dtype=torch.float)\n",
        "                grapheme_root = data[\"grapheme_root\"].to(device, dtype=torch.long)\n",
        "                vowel_diacritic = data[\"vowel_diacritic\"].to(device, dtype=torch.long)\n",
        "                consonant_diacritic = data[\"consonant_diacritic\"].to(device, dtype=torch.long)\n",
        "                \n",
        "                targets = (grapheme_root, vowel_diacritic, consonant_diacritic)\n",
        "                outputs = model(image)\n",
        "                loss = loss_fc(outputs, targets)\n",
        "                \n",
        "                val_loss += loss.item()\n",
        "                val_batches += 1\n",
        "            except Exception as e:\n",
        "                continue\n",
        "    \n",
        "    avg_val_loss = val_loss / max(val_batches, 1)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    \n",
        "    scheduler.step(avg_val_loss)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Save checkpoint\n",
        "    checkpoint = {\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state_dict': model.module.state_dict() if hasattr(model, 'module') else model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses\n",
        "    }\n",
        "    torch.save(checkpoint, f'models/ocr_model_epoch_{epoch+1}.pth')\n",
        "    print(f\"Saved checkpoint: models/ocr_model_epoch_{epoch+1}.pth\")\n",
        "\n",
        "print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Load Trained Model for Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the best model checkpoint\n",
        "def load_model(checkpoint_path='models/ocr_model_epoch_5.pth'):\n",
        "    model = resnet34().to(device)\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        if 'model_state_dict' in checkpoint:\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            print(f\"Loaded model from {checkpoint_path}\")\n",
        "        else:\n",
        "            model.load_state_dict(checkpoint)\n",
        "            print(f\"Loaded model weights from {checkpoint_path}\")\n",
        "    else:\n",
        "        print(f\"Checkpoint not found: {checkpoint_path}\")\n",
        "        print(\"Using untrained model\")\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Uncomment to load a specific checkpoint\n",
        "# model = load_model('models/ocr_model_epoch_5.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Character Recognition Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recognize_character(patch, model, grapheme_maps):\n",
        "    \"\"\"\n",
        "    Recognize a single character patch\n",
        "    Args:\n",
        "        patch: numpy array or PIL Image of the character\n",
        "        model: trained ResNet34 model\n",
        "        grapheme_maps: dictionary with character mappings\n",
        "    Returns:\n",
        "        recognized_char: combined character string\n",
        "        confidence: confidence scores\n",
        "    \"\"\"\n",
        "    # Preprocess image\n",
        "    if isinstance(patch, np.ndarray):\n",
        "        img = Image.fromarray(patch)\n",
        "    else:\n",
        "        img = patch\n",
        "    \n",
        "    # Enhance contrast\n",
        "    enhancer = ImageEnhance.Contrast(img)\n",
        "    img_enhanced = enhancer.enhance(2.0)\n",
        "    img_inv = ImageOps.invert(img_enhanced)\n",
        "    img_inv = img_inv.resize((224, 128)).convert('RGB')\n",
        "    \n",
        "    # Transform\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "    \n",
        "    img_tensor = transform(img_inv).unsqueeze(0).to(device)\n",
        "    \n",
        "    # Inference\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img_tensor)\n",
        "        \n",
        "        # Get predictions\n",
        "        root_idx = torch.argmax(outputs[0], dim=1).item()\n",
        "        vowel_idx = torch.argmax(outputs[1], dim=1).item()\n",
        "        cons_idx = torch.argmax(outputs[2], dim=1).item()\n",
        "        \n",
        "        # Get confidence scores\n",
        "        root_conf = torch.softmax(outputs[0], dim=1)[0][root_idx].item()\n",
        "        vowel_conf = torch.softmax(outputs[1], dim=1)[0][vowel_idx].item()\n",
        "        cons_conf = torch.softmax(outputs[2], dim=1)[0][cons_idx].item()\n",
        "        \n",
        "        # Map to characters\n",
        "        root_char = grapheme_maps['grapheme_root'].get(str(root_idx), '?')\n",
        "        vowel_char = grapheme_maps['vowel_diacritic'].get(str(vowel_idx), '')\n",
        "        cons_char = grapheme_maps['consonant_diacritic'].get(str(cons_idx), '')\n",
        "        \n",
        "        recognized_char = root_char + vowel_char + cons_char\n",
        "        confidence = (root_conf + vowel_conf + cons_conf) / 3\n",
        "        \n",
        "    return recognized_char, confidence, {\n",
        "        'root': (root_char, root_conf),\n",
        "        'vowel': (vowel_char, vowel_conf),\n",
        "        'consonant': (cons_char, cons_conf)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Test on Sample Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on a sample image from the dataset\n",
        "def test_sample_image(image_path=None):\n",
        "    if image_path is None:\n",
        "        # Use first image from test set\n",
        "        test_dataset = BanglaGraphemeDataset(IMG_H, IMG_W, 'test')\n",
        "        if len(test_dataset) > 0:\n",
        "            sample = test_dataset[0]\n",
        "            image = sample['image']\n",
        "            # Convert tensor back to PIL for display\n",
        "            img_array = (image.permute(1, 2, 0).numpy() + 1) / 2 * 255\n",
        "            img_array = img_array.astype(np.uint8)\n",
        "            img_pil = Image.fromarray(img_array)\n",
        "        else:\n",
        "            print(\"No test images available\")\n",
        "            return\n",
        "    else:\n",
        "        img_pil = Image.open(image_path).convert('RGB')\n",
        "        img_pil = img_pil.resize((IMG_W, IMG_H))\n",
        "    \n",
        "    # Recognize\n",
        "    char, conf, details = recognize_character(img_pil, model, grapheme_maps)\n",
        "    \n",
        "    print(f\"Recognized Character: {char}\")\n",
        "    print(f\"Confidence: {conf:.2%}\")\n",
        "    print(f\"Details:\")\n",
        "    print(f\"  Root: {details['root'][0]} ({details['root'][1]:.2%})\")\n",
        "    print(f\"  Vowel: {details['vowel'][0]} ({details['vowel'][1]:.2%})\")\n",
        "    print(f\"  Consonant: {details['consonant'][0]} ({details['consonant'][1]:.2%})\")\n",
        "    \n",
        "    return char, conf, details\n",
        "\n",
        "# Uncomment to test\n",
        "# test_sample_image()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Save Model for Web Application\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the final model\n",
        "def save_final_model(model, save_path='models/ocr_model_final.pth'):\n",
        "    os.makedirs('models', exist_ok=True)\n",
        "    if hasattr(model, 'module'):\n",
        "        torch.save(model.module.state_dict(), save_path)\n",
        "    else:\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Model saved to {save_path}\")\n",
        "\n",
        "# Uncomment to save\n",
        "# save_final_model(model, 'models/ocr_model_final.pth')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
