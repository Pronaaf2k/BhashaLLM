import streamlit as st
import os
import base64
import io
import json
import torch
import numpy as np
from PIL import Image, ImageOps, ImageEnhance
from torchvision import transforms
import google.generativeai as genai
from datetime import datetime
import sys
import time

# Page config
st.set_page_config(
    page_title="BhashaLLM - Bengali Handwriting Recognition",
    page_icon="‚ö°",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# Add paths for model imports
sys.path.append('banglaWrittenWordOCR-main')
sys.path.append('banglaWrittenWordOCR-main/recongnition_model')

# Configuration
MODEL_PATH = 'models/ocr_model_final.pth'
MAPPING_FILE = 'bangla_ocr_pipeline/grapheme_maps.json'

# Initialize session state
if 'recognized_text' not in st.session_state:
    st.session_state.recognized_text = ''
if 'analysis_done' not in st.session_state:
    st.session_state.analysis_done = False
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []

# Load grapheme mappings
@st.cache_data
def load_grapheme_maps():
    if os.path.exists(MAPPING_FILE):
        with open(MAPPING_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    else:
        return {
            "grapheme_root": {str(i): f"root_{i}" for i in range(168)},
            "vowel_diacritic": {str(i): f"vowel_{i}" for i in range(11)},
            "consonant_diacritic": {str(i): f"cons_{i}" for i in range(7)}
        }

grapheme_maps = load_grapheme_maps()

# Load OCR model
@st.cache_resource
def load_ocr_model():
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    try:
        import pretrainedmodels
        import torch.nn as nn
        import torch.nn.functional as F
        
        class resnet34(nn.Module):
            def __init__(self):
                super(resnet34, self).__init__()
                self.model = pretrainedmodels.__dict__["resnet34"](pretrained=None)
                self.l0 = nn.Linear(512, 168)
                self.l1 = nn.Linear(512, 11)
                self.l2 = nn.Linear(512, 7)

            def forward(self, x):
                bs, _, _, _ = x.shape
                x = self.model.features(x)
                x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)
                l0 = self.l0(x)
                l1 = self.l1(x)
                l2 = self.l2(x)
                return l0, l1, l2
        
        model = resnet34().to(device)
        if os.path.exists(MODEL_PATH):
            model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
            st.success(f"‚úÖ Loaded OCR model from {MODEL_PATH}")
        else:
            st.warning(f"‚ö†Ô∏è Model not found at {MODEL_PATH}. Using untrained model.")
        model.eval()
        return model, device
    except Exception as e:
        st.error(f"Error loading model: {e}")
        return None, device

ocr_model, device = load_ocr_model()

# Initialize Gemini
@st.cache_resource
def init_gemini():
    api_key = os.getenv('GEMINI_API_KEY', '')
    # Try to get from Streamlit secrets if available
    if not api_key:
        try:
            api_key = st.secrets.get('GEMINI_API_KEY', '')
        except:
            pass
    if api_key:
        try:
            genai.configure(api_key=api_key)
            return genai.GenerativeModel('gemini-pro')
        except:
            return None
    return None

gemini_model = init_gemini()

def preprocess_image(image):
    """Preprocess image for OCR"""
    if isinstance(image, np.ndarray):
        image = Image.fromarray(image)
    
    # Convert to grayscale and enhance
    if image.mode != 'L':
        image = image.convert('L')
    
    # Enhance contrast
    enhancer = ImageEnhance.Contrast(image)
    image = enhancer.enhance(2.0)
    
    # Invert if needed
    image = ImageOps.invert(image)
    
    # Resize
    image = image.resize((224, 128)).convert('RGB')
    
    return image

def recognize_character(image_patch):
    """Recognize a single character"""
    if ocr_model is None:
        return "?", 0.0, {}
    
    # Transform
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])
    
    img_tensor = transform(image_patch).unsqueeze(0).to(device)
    
    # Inference
    with torch.no_grad():
        outputs = ocr_model(img_tensor)
        
        root_idx = torch.argmax(outputs[0], dim=1).item()
        vowel_idx = torch.argmax(outputs[1], dim=1).item()
        cons_idx = torch.argmax(outputs[2], dim=1).item()
        
        root_conf = torch.softmax(outputs[0], dim=1)[0][root_idx].item()
        vowel_conf = torch.softmax(outputs[1], dim=1)[0][vowel_idx].item()
        cons_conf = torch.softmax(outputs[2], dim=1)[0][cons_idx].item()
        
        root_char = grapheme_maps['grapheme_root'].get(str(root_idx), '?')
        vowel_char = grapheme_maps['vowel_diacritic'].get(str(vowel_idx), '')
        cons_char = grapheme_maps['consonant_diacritic'].get(str(cons_idx), '')
        
        recognized_char = root_char + vowel_char + cons_char
        confidence = (root_conf + vowel_conf + cons_conf) / 3
    
    return recognized_char, confidence, {
        'root': (root_char, root_conf),
        'vowel': (vowel_char, vowel_conf),
        'consonant': (cons_char, cons_conf)
    }

def recognize_word(image):
    """Recognize word from image"""
    start_time = time.time()
    
    # Preprocess
    processed_image = preprocess_image(image)
    
    # Recognize
    char, confidence, details = recognize_character(processed_image)
    
    processing_time = (time.time() - start_time) * 1000
    
    return {
        'recognized_text': char,
        'confidence': confidence,
        'processing_time_ms': int(processing_time),
        'details': details
    }

def get_gemini_response(message, recognized_text, context='tutor'):
    """Get response from Gemini"""
    if not gemini_model:
        return "Gemini API not configured. Please set GEMINI_API_KEY."
    
    if context == 'tutor':
        prompt = f"""You are a helpful Bengali language tutor. The user has written the word "{recognized_text}" in Bengali handwriting.

User question: {message}

Provide a helpful, educational response in Bengali. If the question is about the recognized word, explain its meaning, usage, etymology, or any relevant linguistic information."""
    else:
        prompt = f"""You are providing a philosophical perspective on Bengali literature and culture. The user has written the word "{recognized_text}".

User question: {message}

Provide a thoughtful, philosophical response in Bengali, similar to how Rabindranath Tagore, Kazi Nazrul Islam, or Jasim Uddin might reflect on this word. Make it poetic and profound."""
    
    try:
        response = gemini_model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"Error: {str(e)}"

def get_philosophical_perspectives(recognized_text):
    """Get philosophical perspectives"""
    if not gemini_model:
        return []
    
    perspectives = []
    figures = [
        {'name': 'Rabindranath Tagore', 'style': 'Philosophical', 'prompt': 'philosophical and spiritual'},
        {'name': 'Kazi Nazrul Islam', 'style': 'Passionate', 'prompt': 'passionate and revolutionary'},
        {'name': 'Jasim Uddin', 'style': 'Folk', 'prompt': 'folk and rural life'}
    ]
    
    for figure in figures:
        prompt = f"""Write a poetic reflection on the Bengali word "{recognized_text}" in the style of {figure['name']}. 
        Make it {figure['prompt']}, profound, and beautiful. Write in Bengali, 2-3 sentences."""
        
        try:
            response = gemini_model.generate_content(prompt)
            perspectives.append({
                'name': figure['name'],
                'style': figure['style'],
                'quote': response.text
            })
        except:
            perspectives.append({
                'name': figure['name'],
                'style': figure['style'],
                'quote': f'"{recognized_text}" ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡ßá {figure["name"]} ‡¶è‡¶∞ ‡¶¶‡ßÉ‡¶∑‡ßç‡¶ü‡¶ø‡¶≠‡¶ô‡ßç‡¶ó‡¶ø...'
            })
    
    return perspectives

# Custom CSS
st.markdown("""
<style>
    .main-header {
        text-align: center;
        padding: 20px 0;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border-radius: 10px;
        margin-bottom: 30px;
    }
    .main-header h1 {
        color: white;
        font-size: 2.5rem;
        margin: 0;
    }
    .metric-card {
        background: rgba(255, 255, 255, 0.05);
        padding: 15px;
        border-radius: 10px;
        text-align: center;
        border: 1px solid rgba(255, 255, 255, 0.1);
    }
    .recognized-text {
        font-size: 3rem;
        font-weight: bold;
        text-align: center;
        padding: 20px;
        background: rgba(255, 255, 255, 0.05);
        border-radius: 10px;
        margin: 20px 0;
    }
    .stProgress > div > div > div {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
    }
</style>
""", unsafe_allow_html=True)

# Header
st.markdown("""
<div class="main-header">
    <h1>‚ö° BHASHALLM</h1>
    <p style="color: rgba(255,255,255,0.8); margin: 10px 0;">Bengali Handwriting Recognition System</p>
</div>
""", unsafe_allow_html=True)

# Main layout
col1, col2 = st.columns([1, 1])

with col1:
    st.header("üìù Input Section")
    
    # Input method tabs
    input_method = st.radio("Choose input method:", ["Sketch", "Upload", "Camera"], horizontal=True)
    
    if input_method == "Sketch":
        st.info("üí° Draw Bengali characters on the canvas below")
        # Simple drawing interface
        uploaded_file = st.file_uploader("Or upload an image", type=['png', 'jpg', 'jpeg'])
        if uploaded_file:
            image = Image.open(uploaded_file).convert('RGB')
            st.image(image, caption="Uploaded Image", use_container_width=True)
        else:
            st.warning("Please upload an image or use a drawing tool")
            image = None
    
    elif input_method == "Upload":
        uploaded_file = st.file_uploader("Upload Bengali handwriting image", type=['png', 'jpg', 'jpeg'])
        if uploaded_file:
            image = Image.open(uploaded_file).convert('RGB')
            st.image(image, caption="Uploaded Image", use_container_width=True)
        else:
            image = None
    
    else:  # Camera
        uploaded_file = st.camera_input("Take a picture")
        if uploaded_file:
            image = Image.open(uploaded_file).convert('RGB')
            st.image(image, caption="Captured Image", use_container_width=True)
        else:
            image = None
    
    # Analyze button
    if image and st.button("üîç Analyze Text", type="primary", use_container_width=True):
        with st.spinner("Analyzing handwriting..."):
            result = recognize_word(image)
            
            st.session_state.recognized_text = result['recognized_text']
            st.session_state.analysis_done = True
            st.session_state.result = result
            
            st.success("Analysis complete!")
            st.rerun()
    
    # Chatbot section
    st.header("üí¨ Chatbot Section")
    
    if st.session_state.analysis_done:
        context = st.selectbox("Select mode:", ["Tutor", "Philosophical"], key="context_select")
        
        # Chat history
        for msg in st.session_state.chat_history:
            with st.chat_message(msg["role"]):
                st.write(msg["content"])
        
        # Chat input
        user_input = st.chat_input("Ask anything about the recognized text...")
        
        if user_input:
            # Add user message
            st.session_state.chat_history.append({"role": "user", "content": user_input})
            
            # Get response
            with st.spinner("Thinking..."):
                response = get_gemini_response(
                    user_input, 
                    st.session_state.recognized_text,
                    context.lower()
                )
                st.session_state.chat_history.append({"role": "assistant", "content": response})
            
            st.rerun()
        
        # Suggested follow-ups
        st.subheader("üí° Suggested Follow-ups")
        followups = [
            "‡¶è‡¶á ‡¶∂‡¶¨‡ßç‡¶¶‡¶ü‡¶ø‡¶∞ ‡¶¨‡ßç‡¶Ø‡ßÅ‡ßé‡¶™‡¶§‡ßç‡¶§‡¶ø ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡ßá ‡¶¨‡¶≤‡ßÅ‡¶®",
            "‡¶è‡¶á ‡¶π‡¶∏‡ßç‡¶§‡¶æ‡¶ï‡ßç‡¶∑‡¶∞‡ßá‡¶∞ ‡¶∂‡ßà‡¶≤‡ßÄ‡¶ó‡¶§ ‡¶¨‡ßà‡¶∂‡¶ø‡¶∑‡ßç‡¶ü‡ßç‡¶Ø ‡¶ï‡ßÄ?",
            "‡¶è‡¶á ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø‡¶ø‡¶ï ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡ßá ‡¶Ü‡¶≤‡ßã‡¶ö‡¶®‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®"
        ]
        
        for followup in followups:
            if st.button(followup, key=f"followup_{followup}", use_container_width=True):
                st.session_state.chat_history.append({"role": "user", "content": followup})
                with st.spinner("Thinking..."):
                    response = get_gemini_response(
                        followup,
                        st.session_state.recognized_text,
                        context.lower()
                    )
                    st.session_state.chat_history.append({"role": "assistant", "content": response})
                st.rerun()
    else:
        st.info("üëÜ Please analyze an image first to start chatting")

with col2:
    st.header("üìä Analysis Results")
    
    if st.session_state.analysis_done and 'result' in st.session_state:
        result = st.session_state.result
        
        # Recognized content
        st.subheader("‚úÖ Recognized Content")
        st.markdown(f'<div class="recognized-text">{st.session_state.recognized_text}</div>', unsafe_allow_html=True)
        
        # Metrics
        col_a, col_b, col_c, col_d = st.columns(4)
        
        with col_a:
            st.metric("‚è± TIME", f"{result['processing_time_ms']}ms")
        
        with col_b:
            st.metric("üìù TYPE", "Text")
        
        with col_c:
            st.metric("# CHARS", len(st.session_state.recognized_text))
        
        with col_d:
            confidence_pct = result['confidence'] * 100
            st.metric("‚úì CONFIDENCE", f"{confidence_pct:.1f}%")
        
        # Structural metrics
        st.subheader("üìà Structural Metrics")
        
        stroke_quality = 100
        linearity = 89
        complexity = 24
        
        st.progress(stroke_quality / 100, text=f"STROKE QUALITY: {stroke_quality}%")
        st.progress(linearity / 100, text=f"LINEARITY: {linearity}%")
        st.progress(complexity / 100, text=f"COMPLEXITY: {complexity}%")
        
        # Literary perspectives
        st.subheader("üìö Literary Perspectives")
        
        if gemini_model:
            with st.spinner("Loading perspectives..."):
                perspectives = get_philosophical_perspectives(st.session_state.recognized_text)
            
            for perspective in perspectives:
                with st.expander(f"üë§ {perspective['name']} ({perspective['style']})"):
                    st.write(perspective['quote'])
        else:
            st.warning("Gemini API not configured. Cannot load perspectives.")
        
        # Prediction distribution
        st.subheader("üéØ Prediction Distribution")
        
        predictions = [
            {'text': result['recognized_text'], 'confidence': result['confidence'] * 100, 'is_top': True},
            {'text': '‡¶≠‡ßÄ‡¶§', 'confidence': 1.0, 'is_top': False},
            {'text': '‡¶≠‡¶æ‡¶á', 'confidence': 0.5, 'is_top': False}
        ]
        
        for pred in predictions:
            label = f"{pred['text']} {'(TOP MATCH)' if pred['is_top'] else ''}"
            st.progress(pred['confidence'] / 100, text=f"{label}: {pred['confidence']:.1f}%")
    
    else:
        st.info("üëà Upload and analyze an image to see results here")

# Sidebar
with st.sidebar:
    st.header("‚öôÔ∏è Settings")
    
    if gemini_model:
        st.success("‚úÖ Gemini API Connected")
    else:
        st.error("‚ùå Gemini API Not Configured")
        st.info("Set GEMINI_API_KEY in environment variables or Streamlit secrets")
    
    if ocr_model:
        st.success("‚úÖ OCR Model Loaded")
    else:
        st.warning("‚ö†Ô∏è OCR Model Not Available")
    
    st.markdown("---")
    st.markdown("### üìñ About")
    st.markdown("""
    **BhashaLLM** is a Bengali handwriting recognition system that combines:
    - **OCR Model**: ResNet34 for character recognition
    - **Gemini AI**: For tutor and philosophical responses
    
    Draw or upload Bengali handwriting to get started!
    """)
    
    if st.button("üîÑ Clear All", use_container_width=True):
        st.session_state.recognized_text = ''
        st.session_state.analysis_done = False
        st.session_state.chat_history = []
        st.rerun()
